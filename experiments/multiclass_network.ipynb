{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclass Classification with a neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.join(os.getcwd(), \"..\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyldl.activations import Sigmoid, Softmax, Tanh\n",
    "from pyldl.layers import Sequential\n",
    "from pyldl.losses import CrossEntropyLoss\n",
    "from pyldl.modules import Linear\n",
    "from pyldl.optimizers import SGD\n",
    "from pyldl.utils import encode_one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "\n",
    "digits = load_digits()\n",
    "\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "y_one_hot = encode_one_hot(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = Sequential(Linear(X.shape[1], 128), Sigmoid(), Linear(128, 64), Sigmoid(), Linear(64, 10), Softmax())\n",
    "loss = CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (64,) (64,10) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mSGD\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_one_hot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\PyLDL\\experiments\\..\\pyldl\\optimizers.py:26\u001b[0m, in \u001b[0;36mSGD\u001b[1;34m(network, loss, X, y, batch_size, gradient_step, n_iter)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_iter):\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (batch_X, batch_Y) \u001b[38;5;129;01min\u001b[39;00m batches:\n\u001b[1;32m---> 26\u001b[0m         \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_X\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_Y\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\PyLDL\\experiments\\..\\pyldl\\optimizers.py:17\u001b[0m, in \u001b[0;36mOptim.step\u001b[1;34m(self, batch_x, batch_y)\u001b[0m\n\u001b[0;32m     15\u001b[0m batch_yhat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network(batch_x)\n\u001b[0;32m     16\u001b[0m delta \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loss\u001b[38;5;241m.\u001b[39mbackward(batch_y, batch_yhat)\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_network\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network\u001b[38;5;241m.\u001b[39mupdate_parameters(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_step)\n",
      "File \u001b[1;32md:\\PyLDL\\experiments\\..\\pyldl\\layers.py:20\u001b[0m, in \u001b[0;36mSequential.backward\u001b[1;34m(self, input, delta)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules))):\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules[i]\u001b[38;5;241m.\u001b[39mbackward_update_gradient(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inputs[i], delta)\n\u001b[1;32m---> 20\u001b[0m     delta \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_modules\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward_delta\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inputs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelta\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\PyLDL\\experiments\\..\\pyldl\\activations.py:29\u001b[0m, in \u001b[0;36mSoftmax.backward_delta\u001b[1;34m(self, input, delta)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbackward_delta\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, delta):\n\u001b[0;32m     28\u001b[0m     softmax \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m---> 29\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdelta\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msoftmax\u001b[49m \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m softmax)\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (64,) (64,10) "
     ]
    }
   ],
   "source": [
    "SGD(network, loss, X, y_one_hot, batch_size=64, gradient_step=1e-3, n_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X):\n",
    "    return network.forward(X).argmax(axis=1)\n",
    "\n",
    "def accuracy(y, yhat):\n",
    "    return (y == yhat).mean()\n",
    "\n",
    "yhat = predict(X)\n",
    "print(f\"Accuracy: {accuracy(y, yhat)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
